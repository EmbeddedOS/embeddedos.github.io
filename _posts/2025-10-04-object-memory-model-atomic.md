---
title: "Low latency programming: Memory model and atomic operations."
description: >-
  The most important feature in modern C++: new awareness multithreading memory model.

author: Cong
date: 2025-10-04 00:01:00 +0800
categories: [low-latency, memory-model]
tags: [low-latency, oop, memory-model, atomic, cpp]
image:
  path: assets/img/memory_model.png
  alt: memory model.
published: true
---

## 1. Memory model

All data in c++ is made up by *objects*. The C++ standards defines an object as *a region of storage*, so whatever the object type, it's stored at one or more memory locations. Let's visualize an object into memory view.

```text
        Object in user view                         Memory location
┌──────────────────────────────────┐            ┌───────────────────────┐
│ struct my_data                   │            │           i           │
│ {                                │            ├───────────────────────┤
│   int i;                         │            │           d           │
│   double d;                      │            ├───────────────────────┤
│   unsigned bf1:10;               │            │   bf1   │    bf2      │
│   int bf2:25;                    │            ├───────────────────────┤
│   int bf3:0;                     │===========>│          bf3          │
│   int bf4:9;                     │            ├───────────────────────┤
│   int i2;                        │            │          bf4          │
│   char c1, c2;                   │            ├───────────────────────┤
│   std::string s;                 │            │           i2          │
│ };                               │            ├───────────────────────┤
└──────────────────────────────────┘            │           c1          │
                                                ├───────────────────────┤
                                                │           c2          │
                                                ├───────────────────────┤
                                                │           s           │
                                                └───────────────────────┘
```

Methods in C++ are organized in a different way, there are two types of methods: non-virtual and virtual methods. non virtual methods are treated as normal functions by the compiler:

```text
┌────────────────────┐
│ struct my_object   │
│ {                  │
│    int x;          │      ┌───────Text memory segment───────┐
│    void add(int y) │=====>│ void add(my_object *obj, int y) │
│    {               │      │ {                               │
│        x += y;     │      │       obj->x += y;              │
│    }               │      │ }                               │
│ };                 │      └─────────────────────────────────┘
└────────────────────┘
```

Meanwhile virtual methods require extra memory to look at the V-Table at runtime. The `vptr` is added to the object's memory location by the compiler automatically. It holds pointers to the actual method address.

```text
                                           Memory
                                          Location
┌──────────────────────────────────┐    ┌──────────┐    ┌Text memory segment─┐
│ struct my_object                 │===>│   vptr   │==> │ method(int y)      │
│ {                                │    ├──────────┤    │ {...}              │
│    int x;                        │    │  x(int)  │    └────────────────────┘
│    virtual void method(int y);   │    └──────────┘
│ };                               │
└──────────────────────────────────┘
```

## 2. Memory access order

Objects are actually memory regions, so when we access the objects, we actually access memory locations. In that case, we can either do read (load) or write (store) memory. But what is the actually order those memory region can be accessed? there are some point of view about orders:

- The written code order (that's what we wrote).
- The compiler generated code order (the machine code generated by the compiler).
- And the CPU instruction execution order.

Those order can be same, or (mostly) different. Let's get some examples:

```cpp
void func(int &x, int&y)
{
    x += 1;
    y += 10;
    x += 2;
}
```

In this case, the compiler can reorder the instructions to make the code more efficient, for example it can do: `x+=3` and `y+=10` as long as the result is same. Another example:

```cpp
void func(int &x, int&y)
{
    x += 1;
    y = 10 + x;
    x += 2;
}
```

Now the compiler can't reorder statements due to the dependency of `x` in `y`. But when the CPU executes generated instructions, it can execute in different orders. One of the popular is called [out of order execution](https://en.wikipedia.org/wiki/Out-of-order_execution), in short, the CPU may execute loads/stores/ALU operations out of order to improve performance. For example:

```nasm
mov    eax, [var1]  ; [1]: load variable var1 into reg eax
inc    eax          ; [2]: eax += 1
mov    [var1], eax  ; [3]: store reg eax into var1
xor    ecx, ecx     ; [4]: ecx = 0
inc    ecx          ; [5]: ecx += 1
add    eax, ecx     ; [6]: eax = eax + ecx
```

The CPU can do step `[1]` to load the `var1`, while loading, it could issue some of the later instructions such as `[4]` and `5` too to set the `ecx`. Once step `[1]` is ready `[2]`, `[3]` and finally `[6]` will be executed. So the CPU only issues a load operation, doesn't wait, and do other task, that can avoid idling state and maximize the performance.

Those examples prove one point that the code we write can be different with the final instructions executing by the CPU. The CPU, compilers were free to reorder, cache, or optimize variables without any rules about multithreading. That's totally fine — because to your single thread, results are the same. The problems come out when we start working in multithreading, there's no general rules for compilers or CPUs to do that, and neither of them know about different threads. And we need to tell them, that's actually what C++ 11 provide: atomic operations and locking.

## 3. Memory model since C++11

So far as we know, everything hinges on memory locations. If multithread access to the same location, and at least one try to modify, there's a potential of race condition. To avoid that, *there has to be an enforced ordering between the accesses in the threads*. For example, there could be a fixed order, where one thread always run first, or whatever, just make sure there is some defined ordering.

One way to guarantee the order is locking with mutexes. When two threads try to lock the same mutex, one will be blocked until the other unlock it. So we can do the memory access while the mutex is locking without worrying about other's access.

The other way is to use synchronization properties of atomic operations, either on same or different memory region, accessing to those regions is enforced an ordering.

### 3.1. Atomic operations in compiler and CPU perspectives

In multithread environment, if the object isn't a atomic type, you are responsible for making sure that there's a sufficient synchronization to guarantee that threads are agree on the modification order of each variable. But if you do use atomic operations, the compiler is responsible for ensuring the right synchronization is in place. Let's take a look at this example, to see how the compiler treats a atomic variable compare with a normal variable.

```text
    Coder view                      Compiler assembler view
┌────────────────────┐
│ int counter = 0;   │      ┌─────────────────────────────────┐
│ f() {              │=====>│ mov eax, [counter]              │
│    counter++;      │      │ add eax, 1                      │
│ };                 │      │ mov [counter], eax              │
└────────────────────┘      └─────────────────────────────────┘
```

The compiler sees no synchronization and might assume that, only that single thread access this variable, it can be kept in a register and even can do some reorder load/store to make the code more efficient. Now let's see how it assembles an atomic variable (in a x86 machine):

```text
    Coder view                      Compiler assembler view
┌─────────────────────────┐
│ std::atomic<int> = 0;   │      ┌─────────────────────────────────┐
│ f() {                   │=====>│ lock add dword ptr [counter], 1 │
│    counter++;           │      └─────────────────────────────────┘
│ };                      │
└─────────────────────────┘
```

The `lock` prefix tell the CPU: *This instruction must be atomic across all cores and flush necessary store buffers*. Now the responsible is pushed to the CPU and hardware to guarantee no interleaving from other thread. The C++ memory model defines an abstract machine layer to archive independence from any specific CPU, but the feature might not always available on every CPU. And the memory accessing order rules are dependent on the machine too, some might very strictly compare with other. But because it's actually taken care by C++ hardware abstraction layer, this blog will not dig into features of each machine architecture. In a application view, C++ provides us some kind of memory order that we will dive deep into in the few next sections.

### 3.2. C++ atomic standard library

### 3.3. Memory modification order with `std::memory_order`


## 4. Bonus: Atomic for user-defined types
